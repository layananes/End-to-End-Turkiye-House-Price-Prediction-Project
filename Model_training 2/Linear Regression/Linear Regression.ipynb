{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94217aec",
   "metadata": {},
   "source": [
    "I will start by encoding my data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "2fe0f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"final_cleaned_home_price_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed43e37",
   "metadata": {},
   "source": [
    "I will NOT  scale net area and gross area now to avoid data leakage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe0c7a",
   "metadata": {},
   "source": [
    "I will do one hot encoding on Occupancy status, Investment elligibility, Floor location, and Title deed status.\n",
    "\n",
    "I will also save these original values in a pkl file to retrieve later for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "6474ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for investment eligibility\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe_investment = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "ohe_investment.fit(df[['Investment_Eligibility']])\n",
    "\n",
    "encoded = ohe_investment.transform(df[['Investment_Eligibility']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded,\n",
    "    columns=ohe_investment.get_feature_names_out(['Investment_Eligibility']),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Investment_Eligibility'])\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "with open(\"ohe_investment.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_investment, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "fba7e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for title deed\n",
    "\n",
    "ohe_deed = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "ohe_deed.fit(df[['Title_Deed_Status']])\n",
    "\n",
    "encoded = ohe_deed.transform(df[['Title_Deed_Status']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded,\n",
    "    columns=ohe_deed.get_feature_names_out(['Title_Deed_Status']),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Title_Deed_Status'])\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "with open(\"ohe_deed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_deed, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "84efb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for floor location\n",
    "\n",
    "ohe_floor_location = OneHotEncoder(\n",
    "    handle_unknown = 'ignore',\n",
    "    sparse_output = False    \n",
    ")\n",
    "ohe_floor_location.fit(df[['Floor_Location']])\n",
    "encoded = ohe_floor_location.transform(df[['Floor_Location']])\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded,\n",
    "    columns=ohe_floor_location.get_feature_names_out(['Floor_Location']),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Floor_Location'])\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "with open(\"ohe_floor_location.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_floor_location, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "ea2679a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for occupancy status\n",
    "ohe_occupancy = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "ohe_occupancy.fit(df[['Occupancy_Status']])\n",
    "\n",
    "encoded = ohe_occupancy.transform(df[['Occupancy_Status']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded,\n",
    "    columns=ohe_occupancy.get_feature_names_out(['Occupancy_Status']),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Occupancy_Status'])\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "with open(\"ohe_occupancy.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_occupancy, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14444d05",
   "metadata": {},
   "source": [
    "I will do ordinal encoding on Room count, Building age, Heating type, and Bathroom count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "b475e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for room count\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_room = {1:0,2:1,2.5:2,3:3,3.5:4,4:5,\n",
    "                                     4.5:6,5:7,5.5:8,6:9,7:10,8:11,\n",
    "                                     9:12,10:13,11:14,12:15}\n",
    "\n",
    "df['Room_Count_ordinal'] = df['Room_Count'].map(ordinal_room)\n",
    "df = df.drop(columns=['Room_Count'])\n",
    "\n",
    "\n",
    "with open(\"ordinal_room.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ordinal_room, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "449732aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for building age\n",
    "ordinal_age = {'0 (Yeni)':0, '1':1,'2':2,'3':3,'4':4,'5-10':5,'11-15':6,'16-20':7,'21 Ve Üzeri':8}\n",
    "df['Building_Age'] = df['Building_Age'].astype(str)\n",
    "df['Building_Age'] = df['Building_Age'].str.strip()\n",
    "\n",
    "df['Building_Age_ordinal'] = df['Building_Age'].map(ordinal_age)\n",
    "df = df.drop(columns=['Building_Age'])\n",
    "\n",
    "with open(\"ordinal_age.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ordinal_age, f)\n",
    "    # Assuming you are checking the 'Building_Age' column and using the 'ordinal_age' map\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "c280c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for heating type\n",
    "ordinal_heating = {'Isıtma Yok':0,'Sobalı':1,'Merkezi Kömür':2,'Doğalgaz Sobalı':3,\n",
    "                                              'Diğer':4,'Güneş Enerjisi':5,'Kat Kaloriferi':6,'Merkezi Doğalgaz':7,\n",
    "                                              'Merkezi (Pay Ölçer)':8,'Kombi Doğalgaz':9,'Klimalı':10,'Yerden Isıtma':11,'Jeotermal':12}\n",
    "df['Heating_Type'] = df['Heating_Type'].astype(str)\n",
    "df['Heating_Type'] = df['Heating_Type'].str.strip()\n",
    "\n",
    "df['Heating_Type_ordinal'] = df['Heating_Type'].map(ordinal_heating)\n",
    "df = df.drop(columns=['Heating_Type'])\n",
    "\n",
    "with open(\"ordinal_heating.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ordinal_heating, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "92f9add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bathroom count\n",
    "\n",
    "ordinal_bathroom = {'0.0':0,'1.0':1,'2.0':2,'3.0':3,'4.0':4,'5.0':5,'6.0':6}\n",
    "df['Bathroom_Count'] = df['Bathroom_Count'].astype(str)\n",
    "df['Bathroom_Count'] = df['Bathroom_Count'].str.strip()\n",
    "\n",
    "\n",
    "df['Bathroom_Count_ordinal'] = df['Bathroom_Count'].map(ordinal_bathroom)\n",
    "df = df.drop(columns=['Bathroom_Count'])\n",
    "\n",
    "with open(\"ordinal_bathroom.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ordinal_bathroom, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e0f99",
   "metadata": {},
   "source": [
    "I will now split my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "0ae306c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.drop(columns = ['Price'])\n",
    "y = df['Price']\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size = 0.3,\n",
    "    random_state=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "f1dfb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp,\n",
    "    y_temp,\n",
    "    test_size = 0.5,\n",
    "    random_state = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd30a",
   "metadata": {},
   "source": [
    "I will now scale my columns, and apply target smoothing on city BEFORE training my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "f7918b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train_numerical = x_train[['Net_Area', 'Gross_Area']]\n",
    "\n",
    "scaled_net_gross = StandardScaler()\n",
    "scaled_net_gross.fit(x_train_numerical)\n",
    "\n",
    "with open('scaled_net_gross.pkl', 'wb') as file:\n",
    "    pickle.dump(scaled_net_gross, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93839ffd",
   "metadata": {},
   "source": [
    "I will apply target smoothing on my city column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "b817fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "target_city = TargetEncoder(\n",
    "    min_samples_leaf=20, \n",
    "    smoothing=10.0 )\n",
    "\n",
    "target_city.fit(x_train[['City']], y_train)\n",
    "\n",
    "x_train_city = target_city.transform(x_train[['City']]).values\n",
    "\n",
    "\n",
    "with open('target_city.pkl', 'wb') as file:\n",
    "    pickle.dump(target_city, file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "2a97aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_city = StandardScaler()\n",
    "scaled_city.fit(x_train_city)\n",
    "with open('scaled_city.pkl', 'wb') as file:\n",
    "    pickle.dump(scaled_city, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "3567c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "city_pipeline_steps = [\n",
    "    ('target_smooth', TargetEncoder(min_samples_leaf=20, smoothing=10.0)),\n",
    "    ('scaler', StandardScaler())\n",
    "]\n",
    "\n",
    "city_transform_pipeline = Pipeline(city_pipeline_steps)\n",
    "\n",
    "city_transform_pipeline.fit(x_train[['City']], y_train)\n",
    "\n",
    "#x_test_city_transformed = city_transform_pipeline.transform(x_test[['City']])\n",
    "\n",
    "x_train = x_train.drop('City', axis=1)\n",
    "x_train['City_Target_Scaled'] = pd.DataFrame(x_train_city_transformed, \n",
    "                                            index=x_train.index)\n",
    "\n",
    "x_test = x_test.drop('City', axis=1)\n",
    "x_test['City_Target_Scaled'] = pd.DataFrame(x_test_city_transformed, \n",
    "                                          index=x_test.index)\n",
    "                                          \n",
    "with open('target_scaled_city', 'wb') as file:\n",
    "    pickle.dump(city_transform_pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1cc9e",
   "metadata": {},
   "source": [
    "I will transform my test and train data before merging all pkl files together in a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591d824",
   "metadata": {},
   "source": [
    "for train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "e2c73d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\ndef load_transformer(filename):\\n    with open(filename, 'rb') as file:\\n        return pickle.load(file)\\n\\n\\nnet_gross_scaler = load_transformer('scaled_net_gross.pkl')\\n\\n\\ncolumns_to_transform = ['Net_Area','Gross_Area']\\nx_train[columns_to_transform] = net_gross_scaler.transform(x_train[columns_to_transform])\\nx_test[columns_to_transform] = net_gross_scaler.transform(x_test[columns_to_transform])\""
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def load_transformer(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "\n",
    "net_gross_scaler = load_transformer('scaled_net_gross.pkl')\n",
    "\n",
    "\n",
    "columns_to_transform = ['Net_Area','Gross_Area']\n",
    "x_train[columns_to_transform] = net_gross_scaler.transform(x_train[columns_to_transform])\n",
    "x_test[columns_to_transform] = net_gross_scaler.transform(x_test[columns_to_transform])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbfd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformer(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "city_transform_pipeline = load_transformer('target_scaled_city')\n",
    "net_gross_scaler = load_transformer('scaled_net_gross.pkl')\n",
    "\n",
    "x_val_city_transformed = city_transform_pipeline.transform(x_val[['City']])\n",
    "\n",
    "x_val = x_val.drop('City', axis=1)\n",
    "\n",
    "x_val['City_Target_Scaled'] = pd.Series(\n",
    "    x_val_city_transformed[:, 0], \n",
    "    index=x_val.index\n",
    ")\n",
    "\n",
    "columns_to_transform = ['Net_Area', 'Gross_Area']\n",
    "x_val[columns_to_transform] = net_gross_scaler.transform(x_val[columns_to_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "8e25a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "X_test_scaled = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "2bee5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_test = np.log1p(y_test)\n",
    "y_val = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "1b9785ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "084392fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Evaluation Metrics ##\n",
      "R-squared (R2): 0.6225\n",
      "Mean Squared Error (MSE): 0.1327\n",
      "Root Mean Squared Error (RMSE): 0.3642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"\\n## Evaluation Metrics ##\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "cf7229d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Evaluation Metrics ##\n",
      "R-squared (R2): 0.2140\n",
      "Mean Squared Error (MSE): 0.3045\n",
      "Root Mean Squared Error (RMSE): 0.5518\n"
     ]
    }
   ],
   "source": [
    "y_val_pred_orig_scale = np.expm1(y_val)\n",
    "y_pred2 = model.predict(x_val)\n",
    "r2 = r2_score(y_val, y_pred2)\n",
    "mse = mean_squared_error(y_val, y_pred2)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"\\n## Evaluation Metrics ##\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "5a05e2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score:  0.6446358041678226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('model score: ' ,model.score(x_train,y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815fbb88",
   "metadata": {},
   "source": [
    "strong overfitting, not sure how to combat it :'("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
